"use strict";(self.webpackChunkimdeepmind=self.webpackChunkimdeepmind||[]).push([[3954],{67348:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>o,default:()=>p,frontMatter:()=>l,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"ml/lang-chain/fundamentals","title":"LangChain Fundamentals","description":"Prompt Templates","source":"@site/docs/ml/lang-chain/fundamentals.md","sourceDirName":"ml/lang-chain","slug":"/ml/lang-chain/fundamentals","permalink":"/docs/ml/lang-chain/fundamentals","draft":false,"unlisted":false,"editUrl":"https://github.com/imdeepmind/imdeepmind.github.io/blob/main/docs/ml/lang-chain/fundamentals.md","tags":[],"version":"current","lastUpdatedBy":"Abhishek Chatterjee","lastUpdatedAt":1757836863000,"sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Introduction","permalink":"/docs/ml/lang-chain/introduction"},"next":{"title":"LangChain Agents","permalink":"/docs/ml/lang-chain/agents"}}');var i=t(74848),s=t(28453);const l={sidebar_position:2},o="LangChain Fundamentals",r={},d=[{value:"Prompt Templates",id:"prompt-templates",level:2},{value:"ChatModels",id:"chatmodels",level:2},{value:"Chains",id:"chains",level:2},{value:"Sample code: Summarization using LangChain",id:"sample-code-summarization-using-langchain",level:2},{value:"Explanation of the workflow",id:"explanation-of-the-workflow",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"langchain-fundamentals",children:"LangChain Fundamentals"})}),"\n",(0,i.jsx)(n.h2,{id:"prompt-templates",children:"Prompt Templates"}),"\n",(0,i.jsx)(n.p,{children:"Prompt templates are a fundamental building block in LangChain. They allow developers to define reusable prompts with variables that can be dynamically filled at runtime. Instead of hardcoding prompts as plain strings, prompt templates make the process structured, maintainable, and easier to integrate into applications."}),"\n",(0,i.jsx)(n.p,{children:"For example, a template could be:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"Translate the following text into French: {text}\n"})}),"\n",(0,i.jsxs)(n.p,{children:["When executed with ",(0,i.jsx)(n.code,{children:'{text="Hello, how are you?"}'}),", the final prompt becomes:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"Translate the following text into French: Hello, how are you?\n"})}),"\n",(0,i.jsx)(n.p,{children:"Prompt templates improve consistency, reduce errors, and support scaling applications where prompts need to be reused in different contexts."}),"\n",(0,i.jsx)("div",{style:{textAlign:"center"},children:(0,i.jsx)(n.mermaid,{value:"flowchart LR\n    A[Template with Variables] --\x3e B[Fill Variables at Runtime]\n    B --\x3e C[Final Prompt String]\n    C --\x3e D[Send to LLM]"})}),"\n",(0,i.jsx)(n.h2,{id:"chatmodels",children:"ChatModels"}),"\n",(0,i.jsx)(n.p,{children:"ChatModels are LLMs designed specifically for multi-turn conversations. Unlike base LLMs that take and return plain text, ChatModels handle structured input in the form of messages. Each message has a role (system, user, assistant) and content."}),"\n",(0,i.jsx)(n.p,{children:"This design makes ChatModels more powerful for tasks requiring context, instruction-following, and back-and-forth interactions."}),"\n",(0,i.jsx)(n.p,{children:"Example message structure:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'[\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": "What is LangChain?"},\n    {"role": "assistant", "content": "LangChain is a framework for building LLM applications."}\n]\n'})}),"\n",(0,i.jsx)(n.p,{children:"Key advantages:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Better handling of context across multiple turns"}),"\n",(0,i.jsx)(n.li,{children:"Explicit role separation for instruction vs. query vs. response"}),"\n",(0,i.jsx)(n.li,{children:"Useful in building chatbots, customer support systems, and virtual assistants"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"chains",children:"Chains"}),"\n",(0,i.jsx)(n.p,{children:"Chains are sequences of steps where outputs from one component are fed into the next. They allow combining LLM calls, prompt templates, and external tools into a workflow."}),"\n",(0,i.jsx)(n.p,{children:"For example, a chain could:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Take user input"}),"\n",(0,i.jsx)(n.li,{children:"Fill it into a prompt template"}),"\n",(0,i.jsx)(n.li,{children:"Send it to an LLM"}),"\n",(0,i.jsx)(n.li,{children:"Post-process the result (e.g., parse JSON, store in a database)"}),"\n"]}),"\n",(0,i.jsx)("div",{style:{textAlign:"center"},children:(0,i.jsx)(n.mermaid,{value:"flowchart TD\n    A[User Input] --\x3e B[Prompt Template]\n    B --\x3e C[LLM Call]\n    C --\x3e D[Post-Processing Step]\n    D --\x3e E[Final Output]"})}),"\n",(0,i.jsx)(n.p,{children:"Chains can be:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Simple: one prompt template + one model call"}),"\n",(0,i.jsx)(n.li,{children:"Complex: multiple steps, tools, memory, and branching logic"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"They are essential for building multi-step reasoning pipelines, where the model output guides the next step in the process."}),"\n",(0,i.jsx)(n.h2,{id:"sample-code-summarization-using-langchain",children:"Sample code: Summarization using LangChain"}),"\n",(0,i.jsx)(n.p,{children:"Below is a minimal example showing how to use Prompt Templates, ChatModels, and Chains together to perform text summarization."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from langchain_openai import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\n# Step 1: Define the prompt template\ntemplate = """\nSummarize the following text in 2-3 sentences:\n{text}\n"""\nprompt = PromptTemplate(template=template, input_variables=["text"])\n\n# Step 2: Load a chat model\nchat_model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)\n\n# Step 3: Create a chain combining prompt and model\nsummarization_chain = LLMChain(llm=chat_model, prompt=prompt)\n\n# Step 4: Provide input text\ninput_text = """\nLangChain is an open-source framework that helps developers build applications using large language models.\nIt provides abstractions for prompts, chains, agents, and memory, making it easier to create advanced AI systems.\nLangChain is widely used for chatbots, question answering, document analysis, and workflow automation.\n"""\n\n# Step 5: Run the chain\nsummary = summarization_chain.run({"text": input_text})\nprint(summary)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"explanation-of-the-workflow",children:"Explanation of the workflow"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"PromptTemplate"})," defines the summarization task with a variable ",(0,i.jsx)(n.code,{children:"{text}"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ChatOpenAI"})," loads the chat model (GPT-3.5 in this case)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLMChain"})," connects the prompt with the model, forming a simple chain."]}),"\n",(0,i.jsx)(n.li,{children:"The input text is passed into the chain."}),"\n",(0,i.jsx)(n.li,{children:"The model processes the filled-in prompt and returns a concise summary."}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>o});var a=t(96540);const i={},s=a.createContext(i);function l(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);