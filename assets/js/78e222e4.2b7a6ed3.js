"use strict";(self.webpackChunkimdeepmind=self.webpackChunkimdeepmind||[]).push([[1564],{46578:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"ml/lang-chain/lcel","title":"LCEL","description":"LangChain Expression Language (LCEL) is a declarative way to compose, configure, and run LangChain components. Instead of manually wiring models, prompts, and chains using imperative Python code, LCEL provides a functional and compositional interface. This makes it easier to build, debug, and maintain pipelines that include multiple steps such as prompts, LLMs, retrievers, and custom transformations.","source":"@site/docs/ml/lang-chain/lcel.md","sourceDirName":"ml/lang-chain","slug":"/ml/lang-chain/lcel","permalink":"/docs/ml/lang-chain/lcel","draft":false,"unlisted":false,"editUrl":"https://github.com/imdeepmind/imdeepmind.github.io/blob/main/docs/ml/lang-chain/lcel.md","tags":[],"version":"current","lastUpdatedBy":"Abhishek Chatterjee","lastUpdatedAt":1758354436000,"sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Chains","permalink":"/docs/ml/lang-chain/chains"},"next":{"title":"Tools","permalink":"/docs/ml/lang-chain/tools"}}');var s=i(74848),r=i(28453);const a={sidebar_position:5},l="LCEL",o={},c=[{value:"Core Concepts of LCEL",id:"core-concepts-of-lcel",level:2},{value:"What LCEL is",id:"what-lcel-is",level:3},{value:"Why LCEL matters",id:"why-lcel-matters",level:3},{value:"Flow of LCEL",id:"flow-of-lcel",level:2},{value:"Basic LCEL Example",id:"basic-lcel-example",level:2},{value:"Invoking Pipelines",id:"invoking-pipelines",level:2},{value:"Example of Text Summarization with LCEL",id:"example-of-text-summarization-with-lcel",level:2},{value:"Composition in LCEL",id:"composition-in-lcel",level:2},{value:"Example with Retrieval",id:"example-with-retrieval",level:2},{value:"Benefits of LCEL",id:"benefits-of-lcel",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lcel",children:"LCEL"})}),"\n",(0,s.jsx)(n.p,{children:"LangChain Expression Language (LCEL) is a declarative way to compose, configure, and run LangChain components. Instead of manually wiring models, prompts, and chains using imperative Python code, LCEL provides a functional and compositional interface. This makes it easier to build, debug, and maintain pipelines that include multiple steps such as prompts, LLMs, retrievers, and custom transformations."}),"\n",(0,s.jsxs)(n.p,{children:["LCEL emphasizes ",(0,s.jsx)(n.strong,{children:"composability"}),", ",(0,s.jsx)(n.strong,{children:"clarity"}),", and ",(0,s.jsx)(n.strong,{children:"reusability"}),", enabling us to express workflows in a simple yet powerful manner."]}),"\n",(0,s.jsx)(n.h2,{id:"core-concepts-of-lcel",children:"Core Concepts of LCEL"}),"\n",(0,s.jsx)(n.h3,{id:"what-lcel-is",children:"What LCEL is"}),"\n",(0,s.jsx)(n.p,{children:"LCEL provides a unified API that allows us to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Chain together prompts, LLMs, retrievers, and output parsers."}),"\n",(0,s.jsx)(n.li,{children:"Treat components as composable units."}),"\n",(0,s.jsxs)(n.li,{children:["Execute workflows with a single ",(0,s.jsx)(n.code,{children:".invoke()"}),", ",(0,s.jsx)(n.code,{children:".batch()"}),", or ",(0,s.jsx)(n.code,{children:".stream()"})," call."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"why-lcel-matters",children:"Why LCEL matters"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"It standardizes how we compose pipelines."}),"\n",(0,s.jsx)(n.li,{children:"It provides a consistent interface across components."}),"\n",(0,s.jsx)(n.li,{children:"It supports both synchronous and asynchronous execution."}),"\n",(0,s.jsx)(n.li,{children:"It enables structured debugging by inspecting intermediate states."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"flow-of-lcel",children:"Flow of LCEL"}),"\n",(0,s.jsx)(n.p,{children:"LCEL structures workflows into a pipeline where data flows between components."}),"\n",(0,s.jsx)("div",{style:{textAlign:"center"},children:(0,s.jsx)(n.mermaid,{value:"flowchart LR\n    A[Input] --\x3e B[PromptTemplate]\n    B --\x3e C[LLM/ChatModel]\n    C --\x3e D[OutputParser]\n    D --\x3e E[Final Result]"})}),"\n",(0,s.jsx)(n.p,{children:"This shows that inputs go into a prompt, then to an LLM, then through an output parser, and finally return as structured results."}),"\n",(0,s.jsx)(n.h2,{id:"basic-lcel-example",children:"Basic LCEL Example"}),"\n",(0,s.jsx)(n.p,{children:"A simple LCEL pipeline looks like this:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from langchain.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain.schema import StrOutputParser\n\n# Define a prompt\nprompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")\n\n# Define a model\nmodel = ChatOpenAI(temperature=0)\n\n# Define an output parser\nparser = StrOutputParser()\n\n# Compose them into a pipeline\nchain = prompt | model | parser\n\n# Run the pipeline\nresult = chain.invoke({"topic": "programmers"})\nprint(result)\n'})}),"\n",(0,s.jsx)(n.p,{children:"Here:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"prompt"})," creates the message structure."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"model"})," generates a response."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"parser"})," extracts plain text output."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"|"})," operator chains everything together."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"invoking-pipelines",children:"Invoking Pipelines"}),"\n",(0,s.jsx)(n.p,{children:"LCEL pipelines can be executed in multiple ways:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Single input"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'chain.invoke({"topic": "AI"})\n'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Batch inputs"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'chain.batch([{"topic": "AI"}, {"topic": "Python"}])\n'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Streaming"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'for chunk in chain.stream({"topic": "startups"}):\n    print(chunk, end="")\n'})}),"\n",(0,s.jsx)(n.p,{children:"This flexibility allows LCEL to adapt to different workloads."}),"\n",(0,s.jsx)(n.h2,{id:"example-of-text-summarization-with-lcel",children:"Example of Text Summarization with LCEL"}),"\n",(0,s.jsx)(n.p,{children:"Below is how we can rewrite a summarization task using LCEL."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema import StrOutputParser\n\n# Step 1: Define the prompt\nprompt = ChatPromptTemplate.from_template(\n    "Summarize the following text in 2-3 sentences:\\n{text}"\n)\n\n# Step 2: Load the chat model\nchat_model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)\n\n# Step 3: Define output parser\nparser = StrOutputParser()\n\n# Step 4: Build the LCEL chain\nsummarization_chain = prompt | chat_model | parser\n\n# Step 5: Provide input text\ninput_text = """\nLangChain is an open-source framework that helps developers build applications using large language models.\nIt provides abstractions for prompts, chains, agents, and memory, making it easier to create advanced AI systems.\nLangChain is widely used for chatbots, question answering, document analysis, and workflow automation.\n"""\n\n# Step 6: Run the chain\nsummary = summarization_chain.invoke({"text": input_text})\nprint(summary)\n'})}),"\n",(0,s.jsx)(n.p,{children:"This achieves the same functionality as the non-LCEL approach but with a cleaner, more composable pipeline."}),"\n",(0,s.jsx)(n.h2,{id:"composition-in-lcel",children:"Composition in LCEL"}),"\n",(0,s.jsx)(n.p,{children:"LCEL supports functional composition, which means we can create complex pipelines by joining smaller ones."}),"\n",(0,s.jsx)(n.p,{children:"For example:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Sub-pipeline: question rewriter\nrewrite_prompt = ChatPromptTemplate.from_template("Rephrase the question: {question}")\nrewrite_chain = rewrite_prompt | chat_model | StrOutputParser()\n\n# Sub-pipeline: answer generator\nanswer_prompt = ChatPromptTemplate.from_template("Answer the question: {question}")\nanswer_chain = answer_prompt | chat_model | StrOutputParser()\n\n# Router-style composition\ndef choose_chain(inputs):\n    if "rephrase" in inputs["mode"]:\n        return rewrite_chain\n    return answer_chain\n'})}),"\n",(0,s.jsx)(n.p,{children:"This makes LCEL more expressive than just sequential pipelines."}),"\n",(0,s.jsx)(n.h2,{id:"example-with-retrieval",children:"Example with Retrieval"}),"\n",(0,s.jsx)(n.p,{children:"We can combine LCEL with a retriever to create a Retrieval-Augmented Generation (RAG) pipeline."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from langchain.vectorstores import FAISS\n\n# Assume retriever is already set up\nretriever = FAISS.load_local("my_index").as_retriever()\n\n# Prompt\nrag_prompt = ChatPromptTemplate.from_template(\n    "Answer the question based on context:\\n{context}\\n\\nQuestion: {question}"\n)\n\n# Build pipeline\nrag_chain = {"context": retriever, "question": lambda x: x["question"]} | rag_prompt | chat_model | StrOutputParser()\n\nresult = rag_chain.invoke({"question": "What is LangChain?"})\nprint(result)\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Here we route ",(0,s.jsx)(n.code,{children:"question"})," to both the retriever and the prompt, and then compose everything with LCEL."]}),"\n",(0,s.jsx)(n.h2,{id:"benefits-of-lcel",children:"Benefits of LCEL"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Declarative syntax"})," \u2013 pipelines are defined clearly using ",(0,s.jsx)(n.code,{children:"|"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Composability"})," \u2013 smaller chains can be combined into larger workflows."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Consistency"})," \u2013 the same interface for prompts, models, and tools."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Flexibility"})," \u2013 supports invoke, batch, and streaming modes."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Debugging"})," \u2013 easier to inspect intermediate results."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>l});var t=i(96540);const s={},r=t.createContext(s);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);