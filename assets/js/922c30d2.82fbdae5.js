"use strict";(self.webpackChunkimdeepmind=self.webpackChunkimdeepmind||[]).push([[7288],{26247:(e,i,s)=>{s.r(i),s.d(i,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>l,metadata:()=>n,toc:()=>d});const n=JSON.parse('{"id":"databases/database-systems/lsm-tree","title":"Log-Structured Merge Tree","description":"Log-Structured Merge (LSM) trees are a fundamental data structure used in database storage, particularly for handling high-write workloads efficiently. Here\u2019s a breakdown of key concepts and considerations when working with LSM storage:","source":"@site/docs/databases/database-systems/lsm-tree.md","sourceDirName":"databases/database-systems","slug":"/databases/database-systems/lsm-tree","permalink":"/docs/databases/database-systems/lsm-tree","draft":false,"unlisted":false,"editUrl":"https://github.com/imdeepmind/imdeepmind.github.io/blob/main/docs/databases/database-systems/lsm-tree.md","tags":[],"version":"current","lastUpdatedBy":"Abhishek Chatterjee","lastUpdatedAt":1763216167000,"sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Index Organized Storage","permalink":"/docs/databases/database-systems/index-organized-storage"},"next":{"title":"Buffer Pool Manager","permalink":"/docs/databases/database-systems/buffer-pool-manager"}}');var a=s(74848),t=s(28453);const l={sidebar_position:6},r="Log-Structured Merge Tree",o={},d=[{value:"Concept and Structure",id:"concept-and-structure",level:2},{value:"Write Operation (Write Path)",id:"write-operation-write-path",level:2},{value:"Memtable Insertion",id:"memtable-insertion",level:3},{value:"Write-Ahead Log (WAL)",id:"write-ahead-log-wal",level:3},{value:"Flushing the Memtable",id:"flushing-the-memtable",level:3},{value:"Compaction",id:"compaction",level:3},{value:"Write Amplification",id:"write-amplification",level:3},{value:"Read Operation (Read Path)",id:"read-operation-read-path",level:2},{value:"Lookup in Memtable",id:"lookup-in-memtable",level:3},{value:"Checking SSTables on Disk",id:"checking-sstables-on-disk",level:3},{value:"Bloom Filters",id:"bloom-filters",level:3},{value:"Searching SSTables",id:"searching-sstables",level:3},{value:"Merging Results",id:"merging-results",level:3},{value:"Read Amplification",id:"read-amplification",level:3},{value:"Compaction",id:"compaction-1",level:2},{value:"Strategies",id:"strategies",level:3},{value:"When compaction runs",id:"when-compaction-runs",level:3},{value:"Performance issues with compaction",id:"performance-issues-with-compaction",level:3},{value:"Mitigations and tuning",id:"mitigations-and-tuning",level:3},{value:"Performance Characteristics",id:"performance-characteristics",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"Considerations and Trade-Offs",id:"considerations-and-trade-offs",level:2},{value:"Alternatives and Comparisons",id:"alternatives-and-comparisons",level:2},{value:"MemTable Data Structure",id:"memtable-data-structure",level:2},{value:"SSTable Data Structure",id:"sstable-data-structure",level:2},{value:"LSM Tree Examples",id:"lsm-tree-examples",level:2},{value:"Key-Value and NoSQL Databases",id:"key-value-and-nosql-databases",level:3},{value:"Cassandra",id:"cassandra",level:3},{value:"HBase",id:"hbase",level:3},{value:"RocksDB",id:"rocksdb",level:3},{value:"LevelDB",id:"leveldb",level:3},{value:"ScyllaDB",id:"scylladb",level:3},{value:"DynamoDB (Amazon)",id:"dynamodb-amazon",level:3},{value:"Bigtable (Google)",id:"bigtable-google",level:3},{value:"Relational Databases",id:"relational-databases",level:3},{value:"MyRocks (MySQL with RocksDB)",id:"myrocks-mysql-with-rocksdb",level:3},{value:"CockroachDB",id:"cockroachdb",level:3},{value:"TiDB",id:"tidb",level:3},{value:"<strong>3. Time-Series Databases</strong>",id:"3-time-series-databases",level:2},{value:"InfluxDB",id:"influxdb",level:3},{value:"TimescaleDB",id:"timescaledb",level:3},{value:"<strong>4. Search and Logging Databases</strong>",id:"4-search-and-logging-databases",level:3},{value:"Elasticsearch",id:"elasticsearch",level:3},{value:"Splunk",id:"splunk",level:3},{value:"Log-Structured File Systems (e.g., OpenTSDB)",id:"log-structured-file-systems-eg-opentsdb",level:3},{value:"FAQ: LSM Tree Sizing and Tuning",id:"faq-lsm-tree-sizing-and-tuning",level:2},{value:"How do I select the number of levels?",id:"how-do-i-select-the-number-of-levels",level:3},{value:"How do I choose level sizes and the growth factor (T)?",id:"how-do-i-choose-level-sizes-and-the-growth-factor-t",level:3},{value:"Additional practical tips",id:"additional-practical-tips",level:3}];function c(e){const i={em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.header,{children:(0,a.jsx)(i.h1,{id:"log-structured-merge-tree",children:"Log-Structured Merge Tree"})}),"\n",(0,a.jsx)(i.p,{children:"Log-Structured Merge (LSM) trees are a fundamental data structure used in database storage, particularly for handling high-write workloads efficiently. Here\u2019s a breakdown of key concepts and considerations when working with LSM storage:"}),"\n",(0,a.jsx)("div",{style:{textAlign:"center"},children:(0,a.jsx)(i.mermaid,{value:"flowchart TB\n  subgraph mem [In-Memory]\n    WAL[(WAL - Write Ahead Log)]\n    memtable[(Memtable - In-memory sorted buffer)]\n  end\n  subgraph disk [On-Disk]\n    s0[SSTable L0]\n    s1[SSTable L1]\n    s2[SSTable L2]\n  end\n\n  client --\x3e|write| WAL\n  WAL --\x3e memtable\n  memtable --\x3e|flush SSTable| s0\n  s0 --\x3e|compaction| s1\n  s1 --\x3e|compaction| s2\n  s0 --\x3e|bloom filter| bf0[Bloom Filter L0]\n  s1 --\x3e|bloom filter| bf1[Bloom Filter L1]\n  s2 --\x3e|bloom filter| bf2[Bloom Filter L2]\n\n  classDef io fill:#f9f,stroke:#333,stroke-width:1px;\n  class WAL io;"})}),"\n",(0,a.jsx)(i.h2,{id:"concept-and-structure",children:"Concept and Structure"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Write-Optimized Data Structure"}),": LSM trees are designed to optimize write operations by writing changes sequentially in memory (to a structure known as a memtable), instead of making random writes directly to disk."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Levels and Compaction"}),": Data is stored in multiple levels, and when the memtable reaches a certain size, it\u2019s flushed to disk as a new file (often called an SSTable). As these files accumulate, they are periodically merged and compacted to reduce redundancy, delete obsolete data, and keep read performance efficient."]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"write-operation-write-path",children:"Write Operation (Write Path)"}),"\n",(0,a.jsx)("div",{style:{textAlign:"center"},children:(0,a.jsx)(i.mermaid,{value:"flowchart TB\n  client((Client)) --\x3e|write| wal[WAL]\n  wal --\x3e mem[Memtable]\n  mem --\x3e|flush on threshold| sstL0[SSTable L0]\n  sstL0 --\x3e|compaction| sstL1[SSTable L1]\n  sstL1 --\x3e|compaction| sstL2[SSTable L2]\n  wal -.->|durability| disk"})}),"\n",(0,a.jsx)(i.h3,{id:"memtable-insertion",children:"Memtable Insertion"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Incoming writes (e.g., inserts, updates, deletes) are first placed into a ",(0,a.jsx)(i.strong,{children:"memtable"}),"\u2014an in-memory, sorted data structure."]}),"\n",(0,a.jsx)(i.li,{children:"Writes are handled efficiently in memory, as they avoid random disk I/O."}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"write-ahead-log-wal",children:"Write-Ahead Log (WAL)"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Before data is added to the memtable, it\u2019s written to a ",(0,a.jsx)(i.strong,{children:"write-ahead log (WAL)"}),"."]}),"\n",(0,a.jsx)(i.li,{children:"This ensures durability, as the WAL records each write operation sequentially on disk. If there\u2019s a system crash, the WAL allows the database to recover any uncommitted data."}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"flushing-the-memtable",children:"Flushing the Memtable"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["When the memtable reaches a configured size, it\u2019s flushed to disk as an immutable, sorted file called an ",(0,a.jsx)(i.strong,{children:"SSTable"})," (Sorted String Table)."]}),"\n",(0,a.jsx)(i.li,{children:"This flush process is a sequential write, which is faster and reduces wear on the disk."}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"compaction",children:"Compaction"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Over time, multiple SSTables are created, potentially containing outdated or duplicate records."}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Compaction"})," merges SSTables, removes deleted and old data, and organizes records for better read efficiency."]}),"\n",(0,a.jsx)(i.li,{children:"This reduces the number of files on disk, balancing storage space with performance."}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"write-amplification",children:"Write Amplification"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Frequent compactions increase write amplification, as data may be written to disk multiple times."}),"\n",(0,a.jsx)(i.li,{children:"However, this process ensures that queries remain efficient and storage space is managed effectively."}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"read-operation-read-path",children:"Read Operation (Read Path)"}),"\n",(0,a.jsx)("div",{style:{textAlign:"center"},children:(0,a.jsx)(i.mermaid,{value:"flowchart TB\n  client((Client)) --\x3e|read| mem[Memtable]\n  mem --\x3e|found?| result(Process)\n  mem -.->|not found| check[Search SSTables]\n  check --\x3e|bloom filter says maybe| sstCheck[SSTable Lookup]\n  sstCheck --\x3e|read block| result\n  sstCheck -.->|bloom filter says no| skip[Skip SSTable]"})}),"\n",(0,a.jsx)(i.h3,{id:"lookup-in-memtable",children:"Lookup in Memtable"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["When a read request arrives, the database first checks the ",(0,a.jsx)(i.strong,{children:"memtable"}),"."]}),"\n",(0,a.jsx)(i.li,{children:"Since the memtable is in memory and sorted, it can be quickly searched for the requested key."}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"checking-sstables-on-disk",children:"Checking SSTables on Disk"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["If the key isn\u2019t found in the memtable, the search continues in the ",(0,a.jsx)(i.strong,{children:"SSTables"})," on disk."]}),"\n",(0,a.jsx)(i.li,{children:"SSTables are stored as a series of files on disk, typically organized by levels or tiers to manage access patterns."}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"bloom-filters",children:"Bloom Filters"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Each SSTable has an associated ",(0,a.jsx)(i.strong,{children:"Bloom filter"}),". This probabilistic data structure helps quickly determine whether a key might exist in a specific SSTable."]}),"\n",(0,a.jsx)(i.li,{children:"The Bloom filter check minimizes unnecessary reads by eliminating SSTables that don\u2019t contain the requested key."}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"searching-sstables",children:"Searching SSTables"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"If the Bloom filter indicates that a key might exist in an SSTable, the database reads the relevant SSTable(s)."}),"\n",(0,a.jsx)(i.li,{children:"Within each SSTable, data is sorted, allowing binary search, which is efficient for finding keys."}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"merging-results",children:"Merging Results"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"In cases where the key exists in multiple SSTables (due to updates or deletes), the database merges the results, applying any updates or deletes as necessary."}),"\n",(0,a.jsx)(i.li,{children:"The latest version of the record is returned to the user."}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"read-amplification",children:"Read Amplification"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Searching across multiple SSTables can lead to ",(0,a.jsx)(i.strong,{children:"read amplification"}),", meaning the system may need to access several files to retrieve a single record."]}),"\n",(0,a.jsx)(i.li,{children:"Compaction mitigates this by merging SSTables over time, reducing the number of files that must be checked for each read."}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"compaction-1",children:"Compaction"}),"\n",(0,a.jsx)(i.p,{children:"Compaction merges multiple SSTables into fewer files, removing deleted or outdated entries and thus reducing storage overhead."}),"\n",(0,a.jsx)(i.h3,{id:"strategies",children:"Strategies"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Leveling"}),": SSTables are organized into levels, and compaction involves moving data from one level to the next."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Size-Tiered"}),": Smaller SSTables are periodically merged into larger ones, grouping similar data together to improve access patterns."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"when-compaction-runs",children:"When compaction runs"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Memtable flush triggers"}),": After memtable flushes, new SSTables land in Level 0; when L0 or any level accumulates a threshold number of SSTables, compaction is triggered."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Size thresholds"}),": Compaction is often triggered when the total size of a level exceeds a configured size ratio or threshold compared to the previous level (e.g., Level 1 limit)."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Tombstone/obsolete ratio"}),": If an SSTable contains a high ratio of deleted/obsolete entries (tombstones), compaction will run to clean it up."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Manual/maintenance triggers"}),": Administrators can also trigger compaction manually for maintenance, or systems can run background compactions periodically."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"performance-issues-with-compaction",children:"Performance issues with compaction"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Write amplification"}),": Compaction rewrites data across levels; depending on strategy, compaction increases the amount of data written to disk (K times), which is write amplification."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"I/O contention and latency spikes"}),": Compaction consumes disk I/O and CPU. Without throttling, compaction can cause latency spikes for foreground operations."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"CPU / Memory usage"}),": Compaction requires CPU to merge and rewrite SSTables and memory for buffering; concurrent compactions increase resource usage."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Read amplification during compaction"}),": Compactions can temporarily increase read amplification if background compaction runs while the read pattern needs files being compacted."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Resource balancing trade-offs"}),": Systems must tune compaction threads, throttle I/O, or schedule compaction during off-peak periods to minimize impact."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"mitigations-and-tuning",children:"Mitigations and tuning"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Throttle compaction"}),": Limit compaction I/O throughput to reduce across-the-board impact; allow concurrent compactions with limited threads."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Adaptive compaction"}),": Adjust compaction speed based on system load or target latency SLAs."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Compaction policies"}),": Use leveling for stable read performance (lower read amplification) at the cost of higher write amplification; use size-tiered when aiming to reduce write amplification."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Prioritize hot data"}),": Compaction can prioritize SSTables with high tombstone ratios or those covering hot keys to reduce read/write impact."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Incremental or parallel compaction"}),": Stagger compactions or split work into smaller merges to reduce spikes."]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"performance-characteristics",children:"Performance Characteristics"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Efficient Writes"}),": LSM trees are excellent for high-throughput writes, as they avoid random I/O by writing data in a log-structured fashion."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Read Amplification"}),": Due to multiple levels and compaction, LSM trees can have higher read amplification, meaning reads may need to access several files to retrieve a single key."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Write Amplification"}),": Frequent compaction results in write amplification, which is a trade-off for achieving better read performance over time."]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"High-Write Workloads"}),": LSM trees are ideal for applications with high write demands, like logging, metrics storage, and real-time data analytics."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"NoSQL Databases"}),": Popular databases like Cassandra, HBase, and RocksDB leverage LSM trees for their storage engines due to their write-optimized design."]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"considerations-and-trade-offs",children:"Considerations and Trade-Offs"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Storage Overhead"}),": Frequent flushing and multiple levels of data storage can result in significant storage overhead."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Latency Variability"}),": Compaction can cause spikes in latency, as it is a resource-intensive operation."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Configuration Complexity"}),": Balancing compaction frequency, SSTable size, and the number of levels is crucial for optimizing performance in an LSM-based database system."]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"alternatives-and-comparisons",children:"Alternatives and Comparisons"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"B-Trees"}),": Unlike LSM trees, B-trees optimize for read-heavy workloads by storing data in a hierarchical structure that allows efficient random reads but requires more random I/O for writes."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Hybrid Approaches"}),": Some databases, like MongoDB, offer hybrid options to use either B-trees or LSM-based storage, allowing flexibility based on the workload."]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"memtable-data-structure",children:"MemTable Data Structure"}),"\n",(0,a.jsx)(i.p,{children:"The MemTable, an in-memory structure, is optimized for fast writes and efficient lookups. It typically uses:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Red-Black Tree or AVL Tree"}),": Balanced binary search trees like red-black trees or AVL trees are commonly used for the MemTable. They keep data sorted, allowing efficient in-order traversal and enabling fast reads and writes (in (O(log n)) time)."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Skip List"}),": Some implementations use skip lists as an alternative to trees, especially in write-heavy databases. Skip lists provide probabilistically balanced structure, making inserts, deletes, and lookups efficient (also (O(log n)) time complexity) while being simpler to implement and manage in concurrent environments."]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"sstable-data-structure",children:"SSTable Data Structure"}),"\n",(0,a.jsx)(i.p,{children:"An SSTable, a file stored on disk, is an immutable and sorted structure optimized for sequential read and efficient range queries. Its core components include:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Sorted Array (or Block-Ordered Data)"}),":","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Each SSTable is essentially a sorted array of key-value pairs or records stored sequentially on disk. The entire structure is sorted by keys, allowing efficient binary search within blocks."}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Indexing by Key"}),": Often, SSTables include a simple index (usually loaded into memory) that allows quick lookups by pointing to blocks or offsets within the file for a given key."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Data Blocks"}),":","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"SSTables are divided into data blocks (often 4 KB or 8 KB in size), each containing a subset of sorted records."}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Block-Based Indexing"}),": To reduce I/O operations, an in-memory index is maintained for each SSTable. It contains pointers to the beginning of each block within the SSTable, allowing faster access to specific blocks on disk."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Bloom Filter"}),":","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"SSTables often include a Bloom filter, stored in memory, to quickly check if a specific key might be present in the SSTable."}),"\n",(0,a.jsx)(i.li,{children:"This filter allows the system to skip unnecessary I/O operations by eliminating SSTables that definitely don\u2019t contain the queried key."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"lsm-tree-examples",children:"LSM Tree Examples"}),"\n",(0,a.jsx)(i.h3,{id:"key-value-and-nosql-databases",children:"Key-Value and NoSQL Databases"}),"\n",(0,a.jsx)(i.h3,{id:"cassandra",children:"Cassandra"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Type"}),": Distributed NoSQL database."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Usage"}),": Cassandra is known for its high write and read scalability, making it a popular choice for applications that require high availability and massive amounts of data."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"hbase",children:"HBase"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Type"}),": Columnar NoSQL database built on top of Hadoop."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Usage"}),": Used for managing large amounts of sparse data, HBase uses LSM trees to support fast writes and high data throughput."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"rocksdb",children:"RocksDB"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Type"}),": Embedded key-value store."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Usage"}),": Optimized for fast storage with LSM trees, RocksDB is used within larger systems (like MySQL or Kafka) to provide efficient storage and is popular in storage-heavy applications."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"leveldb",children:"LevelDB"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Type"}),": Embedded key-value store (developed by Google)."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Usage"}),": Uses LSM trees to support fast write operations. It\u2019s often used in desktop and mobile applications due to its simplicity and lightweight design."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"scylladb",children:"ScyllaDB"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Type"}),": NoSQL database (Cassandra-compatible)."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Usage"}),": ScyllaDB is designed for high-performance and low-latency workloads. It uses LSM trees along with a unique architecture that optimizes it for modern hardware."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"dynamodb-amazon",children:"DynamoDB (Amazon)"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Type"}),": Fully managed NoSQL database by AWS."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Usage"}),": DynamoDB is often backed by an LSM-like storage engine optimized for high-write and low-latency applications in a fully managed cloud environment."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"bigtable-google",children:"Bigtable (Google)"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Type"}),": Columnar NoSQL database (managed service on GCP)."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Usage"}),": Known for its scalability and high throughput, Bigtable is used for time-series data, IoT data, and analytical workloads that require efficient write capabilities."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"relational-databases",children:"Relational Databases"}),"\n",(0,a.jsx)(i.h3,{id:"myrocks-mysql-with-rocksdb",children:"MyRocks (MySQL with RocksDB)"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Type"}),": Relational database (MySQL variant)."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Usage"}),": MyRocks is a MySQL storage engine using RocksDB (which uses LSM trees). It\u2019s designed for high-write applications where reducing storage space and write amplification is critical."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"cockroachdb",children:"CockroachDB"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Type"}),": Distributed SQL database."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Usage"}),": Uses LSM trees for its storage layer to manage distributed data, allowing it to handle heavy write operations and maintain high availability across regions."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"tidb",children:"TiDB"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Type"}),": Distributed SQL database."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Usage"}),": TiDB uses an LSM-based storage layer (with RocksDB or TiKV) for high-performance and distributed data management, balancing SQL compatibility with NoSQL performance."]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"3-time-series-databases",children:(0,a.jsx)(i.strong,{children:"3. Time-Series Databases"})}),"\n",(0,a.jsx)(i.h3,{id:"influxdb",children:"InfluxDB"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Type"}),": Time-series database."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Usage"}),": Known for handling high-throughput time-series data, InfluxDB uses an LSM-based storage engine to optimize for frequent writes typical in monitoring, IoT, and analytics applications."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"timescaledb",children:"TimescaleDB"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Type"}),": Time-series database built on PostgreSQL."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Usage"}),": While PostgreSQL uses a B-tree structure by default, TimescaleDB includes LSM options and optimizations for handling high-frequency data insertions in time-series data."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"4-search-and-logging-databases",children:(0,a.jsx)(i.strong,{children:"4. Search and Logging Databases"})}),"\n",(0,a.jsx)(i.h3,{id:"elasticsearch",children:"Elasticsearch"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Type"}),": Search and analytics engine."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Usage"}),": While Elasticsearch primarily uses inverted indices, its underlying data storage and segment merging incorporate LSM-like principles to handle high-ingest data efficiently."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"splunk",children:"Splunk"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Type"}),": Logging and analytics platform."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Usage"}),": Splunk uses an LSM-inspired model for its data storage to support high-speed log ingestion and indexing, essential for real-time analytics and monitoring."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"log-structured-file-systems-eg-opentsdb",children:"Log-Structured File Systems (e.g., OpenTSDB)"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Type"}),": Time-series storage on HBase."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.em,{children:"Usage"}),": OpenTSDB leverages HBase's LSM tree structure to store and retrieve large volumes of time-series data for metrics tracking and monitoring purposes."]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"faq-lsm-tree-sizing-and-tuning",children:"FAQ: LSM Tree Sizing and Tuning"}),"\n",(0,a.jsx)(i.h3,{id:"how-do-i-select-the-number-of-levels",children:"How do I select the number of levels?"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:"The number of levels is determined primarily by your total on-disk dataset size (S), the size of your base level or memtable flush size (M), and the growth factor between levels (T). As a rule-of-thumb for leveling compaction:"}),"\n",(0,a.jsx)(i.p,{children:"L \u2248 ceil(log_T(S / M))"}),"\n",(0,a.jsx)(i.p,{children:"This gives a rough estimate \u2014 actual systems may treat Level 0 differently and use size-limits per level or thresholds for compaction."}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:"Practical guidance:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Choose the memtable / base SSTable size small enough to avoid large L0 write stalls."}),"\n",(0,a.jsx)(i.li,{children:"Use a T value (commonly 8\u201310 for leveling) to make level sizes grow geometrically and keep levels manageable."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"how-do-i-choose-level-sizes-and-the-growth-factor-t",children:"How do I choose level sizes and the growth factor (T)?"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:"Strategy:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Choose a base level size (often determined by memtable size and an L0 cap)."}),"\n",(0,a.jsx)(i.li,{children:"Select a growth factor T where each level is roughly T times the previous level."}),"\n",(0,a.jsx)(i.li,{children:"Larger T reduces the number of levels (and total compaction passes) but increases individual level sizes and can increase read costs for certain patterns."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:"Example: If your base level (L1) target is 1GB and T=10, then L2 target is 10GB, L3 is 100GB and so on."}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:"Tuning trade-offs:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Larger T -> fewer levels -> potentially lower write amplification -> larger compaction work per event -> potentially higher per-compaction latency impact."}),"\n",(0,a.jsx)(i.li,{children:"Smaller T -> more levels -> increased write amplification -> smaller incremental compaction workload -> possibly better read locality."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"additional-practical-tips",children:"Additional practical tips"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Use the library defaults as a starting point (RocksDB default compaction settings and T ~ 10 are sensible for many workloads)."}),"\n",(0,a.jsx)(i.li,{children:"Monitor S, memtable size, L0 file count, and compaction throughput. Tune L0 size limits and compaction concurrency before changing T."}),"\n",(0,a.jsx)(i.li,{children:"Always test under realistic workloads; I/O characteristics and storage type (SSD vs spinning disks) affect optimal settings."}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},28453:(e,i,s)=>{s.d(i,{R:()=>l,x:()=>r});var n=s(96540);const a={},t=n.createContext(a);function l(e){const i=n.useContext(t);return n.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),n.createElement(t.Provider,{value:i},e.children)}}}]);