"use strict";(self.webpackChunkimdeepmind=self.webpackChunkimdeepmind||[]).push([[1827],{45547:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>s,default:()=>c,frontMatter:()=>l,metadata:()=>i,toc:()=>p});const i=JSON.parse('{"id":"ml/lang-chain/promt-templates","title":"PromptTemplates","description":"In LangChain, PromptTemplate is the building block that helps us structure and manage prompts effectively. Instead of writing static strings, we can create dynamic prompts where placeholders are filled with data at runtime. This makes our applications flexible, reusable, and easier to maintain.","source":"@site/docs/ml/lang-chain/promt-templates.md","sourceDirName":"ml/lang-chain","slug":"/ml/lang-chain/promt-templates","permalink":"/docs/ml/lang-chain/promt-templates","draft":false,"unlisted":false,"editUrl":"https://github.com/imdeepmind/imdeepmind.github.io/blob/main/docs/ml/lang-chain/promt-templates.md","tags":[],"version":"current","lastUpdatedBy":"Abhishek Chatterjee","lastUpdatedAt":1758354436000,"sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Introduction","permalink":"/docs/ml/lang-chain/introduction"},"next":{"title":"ChatModels","permalink":"/docs/ml/lang-chain/chat-models"}}');var a=n(74848),r=n(28453);const l={sidebar_position:2},s="PromptTemplates",o={},p=[{value:"Core Concepts of PromptTemplates",id:"core-concepts-of-prompttemplates",level:2},{value:"Purpose of PromptTemplates",id:"purpose-of-prompttemplates",level:3},{value:"Difference from simple string formatting",id:"difference-from-simple-string-formatting",level:3},{value:"Input Variables",id:"input-variables",level:2},{value:"Reusability and Modularity",id:"reusability-and-modularity",level:2},{value:"PromptTemplates in Chains",id:"prompttemplates-in-chains",level:2},{value:"Partial PromptTemplates",id:"partial-prompttemplates",level:2},{value:"Serialization and Storage",id:"serialization-and-storage",level:2},{value:"Example of PromptTemplate with an LLM",id:"example-of-prompttemplate-with-an-llm",level:2},{value:"Key Benefits of PromptTemplates",id:"key-benefits-of-prompttemplates",level:2}];function m(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"prompttemplates",children:"PromptTemplates"})}),"\n",(0,a.jsxs)(t.p,{children:["In LangChain, ",(0,a.jsx)(t.code,{children:"PromptTemplate"})," is the building block that helps us structure and manage prompts effectively. Instead of writing static strings, we can create dynamic prompts where placeholders are filled with data at runtime. This makes our applications flexible, reusable, and easier to maintain."]}),"\n",(0,a.jsxs)(t.p,{children:["A ",(0,a.jsx)(t.code,{children:"PromptTemplate"})," is particularly useful when we need to build prompts that require multiple inputs, serialization, or integration across different components of a system."]}),"\n",(0,a.jsx)(t.h2,{id:"core-concepts-of-prompttemplates",children:"Core Concepts of PromptTemplates"}),"\n",(0,a.jsx)(t.h3,{id:"purpose-of-prompttemplates",children:"Purpose of PromptTemplates"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["They separate the ",(0,a.jsx)(t.strong,{children:"prompt text"})," from the ",(0,a.jsx)(t.strong,{children:"data"})," that goes into it."]}),"\n",(0,a.jsx)(t.li,{children:"They ensure consistency and clarity across multiple LLM calls."}),"\n",(0,a.jsx)(t.li,{children:"They help in debugging and testing since the prompt logic is structured."}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"difference-from-simple-string-formatting",children:"Difference from simple string formatting"}),"\n",(0,a.jsx)(t.p,{children:"While we could directly use Python string formatting, LangChain provides additional features:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Validation of required input variables."}),"\n",(0,a.jsx)(t.li,{children:"Integration with chains, agents, and memory."}),"\n",(0,a.jsx)(t.li,{children:"Easier serialization and storage in JSON/YAML for deployment."}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"input-variables",children:"Input Variables"}),"\n",(0,a.jsxs)(t.p,{children:["A ",(0,a.jsx)(t.code,{children:"PromptTemplate"})," works on the concept of ",(0,a.jsx)(t.strong,{children:"input variables"}),". These are placeholders defined in the template and later replaced with real values when generating prompts."]}),"\n",(0,a.jsx)(t.p,{children:"For example:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from langchain.prompts import PromptTemplate\n\ntemplate = "Translate the following {language} text to English:\\n\\n{text}"\nprompt = PromptTemplate(\n    input_variables=["language", "text"],\n    template=template\n)\n\nfinal_prompt = prompt.format(language="French", text="Bonjour le monde")\nprint(final_prompt)\n'})}),"\n",(0,a.jsx)(t.p,{children:"Output:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-text",children:"Translate the following French text to English:\n\nBonjour le monde\n"})}),"\n",(0,a.jsx)(t.p,{children:"Here:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"language"})," and ",(0,a.jsx)(t.code,{children:"text"})," are input variables."]}),"\n",(0,a.jsxs)(t.li,{children:["The ",(0,a.jsx)(t.code,{children:"format"})," method injects the actual values."]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"reusability-and-modularity",children:"Reusability and Modularity"}),"\n",(0,a.jsx)(t.p,{children:"We often use the same prompt structure for multiple use cases. By defining a template once, we can pass different inputs without rewriting the entire prompt. This is critical when building larger applications such as chatbots, retrieval-augmented generation systems, or custom agents."}),"\n",(0,a.jsx)(t.h2,{id:"prompttemplates-in-chains",children:"PromptTemplates in Chains"}),"\n",(0,a.jsxs)(t.p,{children:["When we use LangChain ",(0,a.jsx)(t.strong,{children:"Chains"}),", the ",(0,a.jsx)(t.code,{children:"PromptTemplate"})," acts as the first step. It ensures that our LLM always receives a well-structured prompt."]}),"\n",(0,a.jsx)("div",{style:{textAlign:"center"},children:(0,a.jsx)(t.mermaid,{value:"flowchart LR\n    A[User Input] --\x3e B[PromptTemplate]\n    B --\x3e C[LLM]\n    C --\x3e D[Response]"})}),"\n",(0,a.jsx)(t.p,{children:"This shows that raw user input goes into the template, which then gets passed to the LLM, producing the final response."}),"\n",(0,a.jsx)(t.h2,{id:"partial-prompttemplates",children:"Partial PromptTemplates"}),"\n",(0,a.jsxs)(t.p,{children:["Sometimes, we want to fix part of the prompt and leave other parts dynamic. LangChain allows us to ",(0,a.jsx)(t.strong,{children:"partially format"})," a template."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'template = "Write a {tone} poem about {topic}"\nprompt = PromptTemplate(\n    input_variables=["tone", "topic"],\n    template=template\n)\n\n# Fix tone as "funny"\npartial_prompt = prompt.partial(tone="funny")\n\nfinal_prompt = partial_prompt.format(topic="programming")\nprint(final_prompt)\n'})}),"\n",(0,a.jsx)(t.p,{children:"Output:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-text",children:"Write a funny poem about programming\n"})}),"\n",(0,a.jsx)(t.p,{children:"This allows us to reuse prompts with preset context while leaving flexibility for the rest."}),"\n",(0,a.jsx)(t.h2,{id:"serialization-and-storage",children:"Serialization and Storage"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.code,{children:"PromptTemplate"})," can be easily serialized to and from dictionaries or JSON. This is useful when we want to store prompt definitions outside our codebase and load them dynamically."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'prompt = PromptTemplate.from_template("Tell me a joke about {subject}")\nprint(prompt.dict())\n'})}),"\n",(0,a.jsx)(t.p,{children:"Output:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-json",children:'{\n  "input_variables": ["subject"],\n  "template": "Tell me a joke about {subject}"\n}\n'})}),"\n",(0,a.jsx)(t.p,{children:"This enables portability and easier integration into larger systems."}),"\n",(0,a.jsx)(t.h2,{id:"example-of-prompttemplate-with-an-llm",children:"Example of PromptTemplate with an LLM"}),"\n",(0,a.jsxs)(t.p,{children:["A full example of using ",(0,a.jsx)(t.code,{children:"PromptTemplate"})," in a chain:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\n\ntemplate = "What is a good name for a company that makes {product}?"\nprompt = PromptTemplate(\n    input_variables=["product"],\n    template=template\n)\n\nchain = LLMChain(llm=llm, prompt=prompt)\n\nresult = chain.run(product="colorful socks")\nprint(result)\n'})}),"\n",(0,a.jsx)(t.p,{children:"Here:"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:["The ",(0,a.jsx)(t.code,{children:"PromptTemplate"})," defines the structure."]}),"\n",(0,a.jsx)(t.li,{children:"The LLMChain binds it with a model."}),"\n",(0,a.jsx)(t.li,{children:"We run the chain with a product input."}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"key-benefits-of-prompttemplates",children:"Key Benefits of PromptTemplates"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Consistency"})," \u2013 same structure for multiple LLM calls."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Reusability"})," \u2013 define once, use everywhere."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Validation"})," \u2013 ensures all required variables are provided."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Integration"})," \u2013 fits neatly into Chains, Agents, and memory systems."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Portability"})," \u2013 easy to serialize and share."]}),"\n"]})]})}function c(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>l,x:()=>s});var i=n(96540);const a={},r=i.createContext(a);function l(e){const t=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),i.createElement(r.Provider,{value:t},e.children)}}}]);