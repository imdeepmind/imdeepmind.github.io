<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="description" content="In this post, I’ll discuss how I processed and prepare the IMDB WIKI dataset for any Machine Learning project.">
		<meta name="robots" content="index, follow" />
		<meta name="author" content="Abhishek Chatterjee (imdeepmind)">
		<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
		<meta http-equiv="X-UA-Compatible" content="ie=edge">

		<link rel="stylesheet" href="/assets/css/base.css">
		<link rel="stylesheet" href="/assets/css/vendor.css">
		<link rel="stylesheet" href="/assets/css/main.css">
		<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
		<link rel="icon" href="/favicon.ico" type="image/x-icon">

		<script src="/assets/js/modernizr.js"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.0.0/pace.min.js" integrity="sha256-V5oQokhQVemIM4vgVPhmy+cTyFEEQhMMvaDOEc7WxJ8=" crossorigin="anonymous"></script>

		<title>Processing IMDB WIKI dataset for action | Blog | imdeepmind | Abhishek Chatterjee</title>
	</head>
	<body id="top">
		<header class="short-header">
			<div class="gradient-block"></div>
			<div class="row header-content">
				<div class="logo">
					<a href="/">Abhishek Chatterjee (imdeepmind)</a>
				</div>
				<nav id="main-nav-wrap">
					<ul class="main-navigation sf-menu">
						<li><a href="/" title="Home">Home</a></li>
						<li><a href="/about/" title="About Me">About</a></li>
						<li class="current"><a href="/blogs" title="My Blogs">Blogs</a></li>
						<li><a href="/projects" title="My Projects">Projects</a></li>
						<li><a href="/datasets/" title="My Datasets">Datasets</a></li>
						<li><a href="/contact/" title="Contact Me">Contact</a></li>
					</ul>
				<div class="search-wrap">
					<form role="search" method="get" class="search-form" action="https://google.com/search">
						<label>
							<span class="hide-content">Search for:</span>
							<input type="search" class="search-field" placeholder="Type Your Keywords" value="" name="q" title="Search for:" autocomplete="off">
						</label>
						<input type="submit" class="search-submit" value="Search">
					</form>
					<a href="#" id="close-search" class="close-btn">Close</a>
				</div>
				<div class="triggers">
					<a class="search-trigger" href="#"><i class="fa fa-search"></i></a>
					<a class="menu-toggle" href="#"><span>Menu</span></a>
				</div>
			</div>
		</header>
		<section id="content-wrap" class="blog-single">
			<div class="row">
				<div class="col-twelve">
					<article class="format-standard">
						<div class="content-media">
							<div class="post-thumb">
								<img src="../../../../assets/images/blogs/hero/imdb_banner.png">
							</div>
						</div>
						<div class="primary-content">
							<h1 class="page-title">Processing IMDB WIKI dataset for action</h1>
							<ul class="entry-meta">
								<li class="date">January 31, 2019 by Abhishek Chatterjee</li>
							</ul>
							<p class="lead">In this post, I’ll discuss how I processed and prepare the IMDB WIKI dataset for any Machine Learning project.</p>

							<p>IMDB WIKI dataset is the largest dataset on human faces. This dataset contains more than 500 thousand + images of human faces with their age, gender, name etc.</p>

                            <p>This dataset very famous among researchers. This dataset can be used various applications like age predictions, gender classification etc.</p>

                            <p>For more information about the dataset please visit <a href="https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/">this website</a>.</p>



							<h2>Understanding the Problem</h2>

							<p>The dataset is great for various projects. It contains more than 500K+ images, so it is perfect for various Deep Learning models. This dataset can also be used for benchmarking various algorithms. </p>

                            <p>But the problem with the dataset is that it contains so many images that do not have any faces. Some of the images are completely corrupted also. </p>

                            <p>So before using this dataset in any project, we need to filter all the images.</p>

                            <p>Also, all the images are in different resolution. Some of the images are greyscaled and some of the images are colored.</p>

                            <p>So that is also a problem.</p>

                            <p>Furthermore, the meta information ( age, name, gender etc) are in .mat files. Files with .mat extension contain MatLab formatted data. </p>

                            <p>Reading these .mat files are little hard in python. 


							<h2>My Solution</h2>

                            <p>I want to use this dataset in one of my projects. But before using it, I need to preprocess it.</p>

                            <p>So I thought that I should create a completely different project just for preprocessing the dataset. By doing that I can don't need to preprocess this dataset for all projects related to this dataset. </p>

                            <p>So I filter the dataset the entire dataset, resize all the images in a fixed size (64x64), convert all the images into greyscale images. </p>

                            <p>Also, I converted all the .mat files into easily readable .csv files. </p>

                            <p>In the end, I flattened all the images and save the images with their meta information in .csv files.</p>


							<h2>How I did it</h2>

                            <h3>So the first step is to convert all the .mat files into .csv files.</h3>

                            <p>To read .mat files in python, we can use Scipy</p>

                            <p><i>This code is for the WIKI dataset. Check the GitHub repo for the IMDB dataset.</i></p>

                            <p>
                                Here is the code for that.

<pre><code>from scipy.io import loadmat

mat_file = "unprocessedData/mat/wiki.mat"
data = loadmat(mat_file)</code></pre>
                            </p>

                            <p>
                                Now the data variable is a dictionary. We don't need all the information. Here is the code for that.
                                <pre><code>data = data['wiki']</code></pre>
                            </p>

                            <p>
                                Now the data variable contains a very nested array. Here is the code for retrieving the data from the variable.

<pre><code>photo_taken = data[0][0][1][0]
full_path = data[0][0][2][0]
gender = data[0][0][3][0]
name = data[0][0][4][0]
face_score1 = data[0][0][6][0]
face_score2 = data[0][0][7][0]</code></pre>
                            </p>

                            <p><b><i>For information about the exact data, these .mat files contain, please visit the home page of the dataset.</i></b></p>

                            <p><b><i>After doing some experiments with the indexing, I figured the proper index of all the data.</i></b></p>

                            <p>I recovered the date of birth of each person from the path of the image. Here is the code for that.

<pre><code>dob = []
for file in path:
    dob.append(file.split('_')[1])</code></pre>
                            </p>

                            <p><b><i>The date of birth information present on the original .mat files are in Matlab serial date number. Honestly, I don't know how to convert these dates on normal dates. So I feel a little comfortable with the approach I did in the code.</i></b></p>



                            <p>
                                    Now different pieces of information recovered from the .mat files need a little more preprocessing. Here is the code for that.

<pre><code>path = []
for file in full_path:
    path.append(file[0])

names = []
for n in name:
    if len(n) > 0:
        names.append(n[0])
    else:
        names.append(np.nan)

genders = []
for n in range(len(gender)):
    if gender[n] == 1:
            genders.append('male')
    else:
    genders.append('female')</code></pre>
                            </p>

                            <p>
                                Finally, we have all the data that we need in the right format, so let's make one multidimensional array holding all the information.
<pre><code>import numpy as np
theData = np.vstack((dob,photo_taken,path,genders,names,face_score1,face_score2)).T</code></pre>
                            </p>

                            <p>
                                    So let's make a DataFrame containing all the information save the DataFrame as a .csv file.
<pre><code>mport pandas as pd

cols = ['dob', 'photo_taken', 'full_path', 'gender', 'name', 'face_score1', 'face_score2']

dataFrame = pd.DataFrame(theData)
dataFrame.columns = cols

dataFrame.to_csv('processedData/mat/wiki/wiki_meta.csv')</code></pre>
                            </p>

                            <h3>The second step is to process all the images and merge them with the meta information.</h3>

                            <p>First lets import all the dependencies.</p>

                            <p><b><i>This code is for the WIKI dataset. Please check the GitHub repo for the IMDB dataset</i></b></p>

                            <p>
<pre><code>import numpy as np
import pandas as pd
import cv2</code></pre>
                            </p>

                            <p>After that, we can read the imdb_meta.csv file
<pre><code>wiki = pd.read_csv('processedData/mat/wiki/wiki_meta.csv')
wiki = wiki.drop(['Unnamed: 0'], axis=1)        </code></pre>
                            </p>

                            <p><b><i>I have a laptop with 8gb RAM and I use this laptop for all my projects. This dataset is big, so I can not process all the images in one shot. So instead, I built a batch system that processes a small chunk of the images at one time. By doing that, the dataset can easily be processed on a system with low RAM.</i></b></p>

                            <p>
                                    So now let's select the information from the wiki_meta.csv file that I need.
<pre><code># Length of the dataset
n = len(wiki)

paths = wiki['full_path'].values.reshape(1,n)[0]
scores = wiki['face_score1'].values.reshape(1,n)[0]
details = wiki.drop(['full_path', 'face_score1', 'face_score2'], axis=1).values</code></pre>
                            </p>


                            <p>
                                    Here are some parameters for the batch system
<pre><code> Size of the batch
BATCH_SIZE = 10000

# Starting from the first batch
BATCH_NUMBER = 1

# Automatically calculated the number of batches based on the BATCH_SIZE and length of the dataset
NO_BATCHES = (n // BATCH_SIZE) + 1

# How many elements are in each batch
ELEMENTS = []</code></pre>
                            </p>

                            <p>
                                    Here is the code for calculating the number of elements on each batch

<pre><code>for i in range(NO_BATCHES):
    if BATCH_SIZE > (n - sum(ELEMENTS)):
            ELEMENTS.append((n - sum(ELEMENTS)))
        else:
            ELEMENTS.append(BATCH_SIZE)</code></pre>
                            </p>

                            <p>Now we have the everything set, so, let’s make the batch system.</p>

                            <p>
                                    First, we need to loop through all the batches. Also, we need to break the loop based the BATCH_NUMBER.

<pre><code>for batch in range(NO_BATCHES):
    if batch < (BATCH_NUMBER-1):
        break
    …
    …</code></pre>
                            </p>

                            <p>
                                    Let’s make two variable start and end inside the batch loop, holding the start and end index in each batch.
<pre><code>start = sum(ELEMENTS[0:batch])
end = sum(ELEMENTS[0:batch+1])        </code></pre>
                            </p>

                            <p>
                                    Now we’ll iterate through all images for preprocessing based on the start and end index.
                            </p>

                            <p>
                                    Also, make a data array for storing all the preprocessed information.
<pre><code>data = []

for i in range(start,end):
	…
	…</code></pre>
                            </p>

                            <p>
                                    And now we’ll read the images using OpenCV and resized them into the proper size, and appending the processed data into the data array.
<pre><code>for i in range(start,end):
    path = 'unprocessedData/images/wiki_crop/' + paths[i]

    print('-- (BATCH-'+ str(batch+1) +')(' + str(i+1) + ') Currently processing image with path ' + path + ' --')

    if scores[i] == -np.inf:
        print('-- (BATCH-'+ str(batch+1) +')(' + str(i+1) + ') Image does not have any face ' + path + ' --')
    else:
        img = cv2.imread(path, 0)
        img = cv2.resize(img, (64,64))
        img = img.reshape(1,4096)

        row = np.hstack((img, details[i].reshape(1,4)))

        data.append(row[0])</code></pre>
                            </p>

                            <p>
                                    Now let's create a DataFrame save the information into a .csv file.
<pre><code>columns = []

for i in range(4096):
    columns.append('pixel' + str(i+1))

    columns = columns + ['dob', 'photo_taken', 'gender', 'name']

    dataFrame = pd.DataFrame(data)

    dataFrame.columns = columns

    dataFrame.to_csv('processedData/images/wiki/wiki_'+str(batch)+'.csv')

    print('-- (BATCH-'+ str(batch+1) +') data save in csv file --')</code></pre>
                            </p>

                            <p>
                                And at the end, we’ll delete the used variables for saving memory.
                                <pre><code>del start, end, data, path, img, row, columns, dataFrame</code></pre>
                            </p>

                            <h2>Future Advancements</h2>

                            <p>We can do a lot more for preprocessing the dataset. This is just the bare minimum.</p>

                            <p>I have many plans to use different algorithms to preprocess the dataset.
                                <ol>
                                    <li>Some images contain multiple faces, we can handle that</li>
                                    <li>We can align faces</li>
                                </ol>
                            </p>
                        </div>
					</article>
				</div>
			</div>
		</section>
		<footer>
			<div class="footer-main">
				<div class="row">
					<div class="col-four tab-full mob-full footer-info">
						<h4>About Me</h4>
						<p>  Hello, I'm Abhishek Chatterjee, creator of NeuralPy: A Keras like
              deep learning library that works on top of PyTorch, a precessional
              Machine Learning Engineer and Web Developer. This is my website,
              and here I share my projects, blogs, etc.</p>
					</div>
					<div class="col-two tab-1-3 mob-1-2 site-links">
						<h4>Site Links</h4>
						<ul>
							<li><a href="/">Home</a></li>
							<li><a href="/about/">About</a></li>
							<li><a href="/blogs/">Blogs</a></li>
							<li><a href="/projects/">Projects</a></li>
							<li><a href="/contact/">Contact</a></li>
						</ul>
					</div>
					<div class="col-two tab-1-3 mob-1-2 social-links">
						<h4>Social</h4>
						<ul>
							<li><a href="https://twitter.com/imdeepmind">Twitter</a></li>
							<li><a href="https://www.linkedin.com/in/imdeepmind/">LinkedIn</a></li>
							<li><a href="https://github.com/imdeepmind/">GitHub</a></li>
						</ul>
					</div>
					<div class="col-four tab-1-3 mob-full footer-subscribe">
						<h4>Other Links</h4>
						<ul>
							<li><a href="https://www.kaggle.com/imdeepmind">Kaggle</a></li>
							<li><a href="https://www.hackerrank.com/imdeepmind">HackerRank</a></li>
						</ul>
					</div>
				</div>
			</div>
			<div class="footer-bottom">
				<div class="row">
					<div class="col-twelve">
						<div class="copyright">
							<span>© Copyright imdeepmind 2018</span>
							<span>Design by <a href="http://www.styleshout.com/">styleshout</a></span>
						</div>
						<div id="go-top">
							<a class="smoothscroll" title="Back to Top" href="#top"><i class="icon icon-arrow-up"></i></a>
						</div>
					</div>
				</div>
			</div>
		</footer>
		<div id="preloader">
			<div id="loader"></div>
		</div>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js" integrity="sha256-IFHWFEbU2/+wNycDECKgjIRSirRNIDp2acEB5fvdVRU=" crossorigin="anonymous"></script>
		<script src="/assets/js/plugins.js"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.appear/0.4.1/jquery.appear.min.js"></script>
		<script src="/assets/js/main.js"></script>
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-124685583-2"></script>
		<script>
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());

			gtag('config', 'UA-124685583-2');
		</script>
	</body>
</html>
